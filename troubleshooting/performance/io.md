

```
stress-ng --hdd 2 --hdd-bytes 1G --timeout 1200s &

```

解释：

- `--hdd 2`：启动 2 个 **hdd worker**，每个 worker 会创建临时文件并不断写入数据。
- `--hdd-bytes 1G`：每个 worker 的文件大小限制为 1GB（写满后会重写）。
- `--timeout 1200s`：运行 20分钟后自动退出。

如果想让它持续写磁盘，可以省略 `--hdd-bytes`，这样会不停写入直到 `timeout`



```
iostat -x 1
Linux 4.18.0-553.el8_10.x86_64 (localhost.localdomain) 	08/25/2025 	_x86_64_	(2 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           3.27    0.03    0.17    0.01    0.01   96.50

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
vda              0.03    0.41      3.02    396.81     0.00     0.00   0.24   0.94    0.34   11.66   0.00   102.98   966.90   0.60   0.03
vdb              0.00    0.00      0.02      0.00     0.00     0.00   0.00   0.00    0.04    0.00   0.00    21.65     0.00   0.12   0.00
scd0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.04    0.00   0.00     3.14     0.00   0.29   0.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          47.76    0.00   26.87    0.50    1.00   23.88

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
vda              0.99  836.63      7.92 833293.56     0.00     2.97   0.00   0.35    0.00   38.48  32.19     8.00   996.01   0.87  73.07
vdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
scd0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          52.26    0.00   37.19    0.00    1.01    9.55

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
vda              0.00 1167.00      0.00 1163077.00     0.00     1.00   0.00   0.09    0.00   11.65  13.59     0.00   996.64   0.30  34.80
vdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
scd0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00


```



## 1️⃣ iostat -x 1 输出分析

### 第一段（空闲状态）

```
avg-cpu:  %user 3.27  %system 0.17  %iowait 0.01  %idle 96.50
vda: w/s=0.41, wkB/s=396.81, svctm=0.60, %util=0.03
```

- CPU 空闲 (`id=96.5%`)，磁盘几乎没压力 (`%util=0.03`)。
- 写入速度极低 (~396KB/s)，I/O 请求响应时间很短 (`svctm=0.6ms`)。
- 此时系统还未进行磁盘压测。

------

### 第二段（压力中）

```
avg-cpu: %user=47.76, %system=26.87, %iowait=0.50
vda: w/s=836.63, wkB/s=833293.56 KB/s, svctm=0.87ms, %util=73.07
```

- CPU 占用增加，系统态（`sy=26.87%`）也显著上升 → 表明磁盘 I/O 消耗了 CPU。
- 写速率非常高：约 **833 MB/s**。
- `%util=73%` → vda 设备约 3/4 时间被忙碌占用，说明 I/O 负载高。
- `w_await=38.48ms` → 磁盘请求平均等待时间 ~38ms，说明磁盘开始排队，但还在可接受范围。
- `rkB/s` 很小 → 压测主要是写操作。

------

### 第三段（压力高峰）

```
avg-cpu: %user=52.26, %system=37.19, %iowait=0
vda: w/s=1167, wkB/s=1163077 KB/s, svctm=0.30ms, %util=34.8
```

- 写速率进一步升高 (~1.1GB/s)，CPU 消耗增加。
- `%util=34.8%` 有点低，可能是因为 I/O 被缓存或批量写操作。
- `w_await=11.65ms` → 平均等待时间下降，可能数据写入被优化或使用了缓存。



```
    vmstat 1
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     1  0      0 543940   3084 1118932    0    0     9  1782   81   57  3  0 96  0  0
     1  0      0 727704   3084 935200    0    0     0 626312 1492  288 33 19 37 10  1
     2  0      0 965640   3084 697324    0    0     0 911820 2109  255 49 30 21  1  0
     1  0      0 553768   3084 1109108    0    0     0 757816 1595  241 38 20 32 11  1
     1  0      0 807080   3084 855860    0    0     0 945225 2181  276 49 32 18  0  0
     1  0      0 568120   3084 1094756    0    0     0 856708 1370  240 28 15 32 25  1
     1  0      0 1052908   3084 610040    0    0     0 856520 2043  296 41 30 28  0  1
```



## vmstat 1 输出分析

```
bi/bo = 1782 -> 945225 -> 856708 KB/s
us/sy/id/wa
us=33~38%, sy=15~32%, wa=10%
```

- `bo`（block out）很高 → 表明磁盘写入压力大。

- `wa=10%` → CPU 等待 I/O 时间增加，但还不算严重瓶颈。

- CPU 被系统态占用较多（`sy` 15~32%），说明大量 I/O 请求处理消耗了 CPU。

  

```
sar -d 1
Linux 4.18.0-553.el8_10.x86_64 (localhost.localdomain) 	08/25/2025 	_x86_64_	(2 CPU)

04:54:30 AM       DEV       tps     rkB/s     wkB/s   areq-sz    aqu-sz     await     svctm     %util
04:54:31 AM  dev252-0    974.00      0.00 952080.00    977.49     12.91     13.25      0.75     72.80
04:54:31 AM dev252-16      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
04:54:31 AM   dev11-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

04:54:31 AM       DEV       tps     rkB/s     wkB/s   areq-sz    aqu-sz     await     svctm     %util
04:54:32 AM  dev252-0    784.00      0.00 802816.00   1024.00      5.14      6.56      0.21     16.10
04:54:32 AM dev252-16      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
04:54:32 AM   dev11-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

04:54:32 AM       DEV       tps     rkB/s     wkB/s   areq-sz    aqu-sz     await     svctm     %util
04:54:33 AM  dev252-0    879.00      0.00 859845.00    978.21     13.11     14.91      0.81     70.90
04:54:33 AM dev252-16      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
04:54:33 AM   dev11-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

```

## sar -d 1 输出分析

```
dev252-0 tps=784~974, wkB/s=802~952 MB/s, await=6~14ms, %util=16~73%
```

- `tps`（每秒 I/O 请求数）很高，说明磁盘 I/O 请求频繁。
- `wkB/s` ≈ 800~950 MB/s → 压测是写操作为主。
- `await=6~14ms` → 请求等待时间适中，磁盘还没严重排队。
- `%util=16~73%` → 不同时间段磁盘利用率波动大，可能和缓存/写合并有关。
- dev252-16 和 dev11-0 没有 I/O 压力，说明只有特定磁盘被 stress-ng 压测。

------

## 4️⃣ 综合分析

1. **磁盘压力大**
   - 写速率高达 800MB~1GB/s。
   - `%util` 高，CPU 系统态增加，`w_await` 有上升趋势 → 磁盘成为主要瓶颈。
2. **CPU 消耗**
   - CPU 用户态和系统态都增加 → I/O 压测消耗 CPU 资源用于处理请求和缓存操作。
3. **等待时间**
   - 平均等待时间在 6~38ms，压力较高但不严重，说明磁盘队列在可控范围内。
4. **系统健康**
   - swap 无使用（vmstat `si/so=0`）。
   - 内存充足 → 磁盘压力主要来自写操作，而非内存不足导致的 page-out。

------

### 🔹 总结结论

- 你的 `stress-ng --hdd` 压测成功，磁盘 I/O 被充分利用。

- vda 是主要受压磁盘，写速率高，CPU 系统态增加。

- 目前没有出现严重 I/O 队列堆积或系统挂起。

- 如果压测目标是查看磁盘瓶颈，可以进一步关注：

  - `%util` 接近 100% → 磁盘成为瓶颈。

  - `w_await` 或 `await` 高于 50~100ms → I/O 延迟大。

    
